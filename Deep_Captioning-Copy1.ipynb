{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning-based Image Captioning with Embedding Reward\n",
    "Originally made by Pranshu Gupta, Deep Learning @ Georgia Institute of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.image_utils import image_from_url\n",
    "from models.nets import PolicyNetwork, RewardNetwork, ValueNetwork\n",
    "from search.search import GenerateCaptions, GenerateCaptionsWithBeamSearch\n",
    "from torchsummary import summary\n",
    "import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: \", device)\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "max_seq_len = 17\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MS-COCO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_coco_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d2bca6be5da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We'll work with dimensionality-reduced features for this notebook, but feel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# free to experiment with the original features by changing the flag below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_coco_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_captions_lens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_captions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_coco_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "data[\"train_captions_lens\"] = np.zeros(data[\"train_captions\"].shape[0])\n",
    "data[\"val_captions_lens\"] = np.zeros(data[\"val_captions\"].shape[0])\n",
    "for i in range(data[\"train_captions\"].shape[0]):\n",
    "    data[\"train_captions_lens\"][i] = np.nonzero(data[\"train_captions\"][i] == 2)[0][0] + 1\n",
    "for i in range(data[\"val_captions\"].shape[0]):\n",
    "    data[\"val_captions_lens\"][i] = np.nonzero(data[\"val_captions\"][i] == 2)[0][0] + 1\n",
    "\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "# for k, v in data.items():\n",
    "#     if type(v) == np.ndarray:\n",
    "#         print(k, type(v), v.shape, v.dtype)\n",
    "#     else:\n",
    "#         print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_coco_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1fd9d53c3969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_coco_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_coco_data' is not defined"
     ]
    }
   ],
   "source": [
    "small_data = load_coco_data(max_train=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU_score(gt_caption, sample_caption, w):\n",
    "    \"\"\"\n",
    "    gt_caption: string, ground-truth caption\n",
    "    sample_caption: string, your model's predicted caption\n",
    "    Returns unigram BLEU score.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        reference = [x for x in gt_caption.split(' ') \n",
    "                     if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "        hypothesis = [x for x in sample_caption.split(' ') \n",
    "                      if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "        \n",
    "    except AttributeError:\n",
    "        reference = [x for x in gt_caption\n",
    "                     if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "        hypothesis = [x for x in sample_caption\n",
    "                      if ('<END>' not in x and '<START>' not in x and '<UNK>' not in x)]\n",
    "        \n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = [w])\n",
    "    return BLEUscore\n",
    "\n",
    "def BLEU_tensor(captions_in, target_caption):\n",
    "    idx_to_word = {i : w for w, i in data[\"word_to_idx\"].items()}\n",
    "    source = []\n",
    "    target = []\n",
    "    \n",
    "    for i in range(captions_in.shape[1]):\n",
    "        source.append(idx_to_word[captions_in[0][i].item()])\n",
    "        \n",
    "    for i in range(target_caption.shape[1]):\n",
    "        target.append(idx_to_word[target_caption[0][i].item()])\n",
    "    \n",
    "    b1 = BLEU_score(source, target,1)\n",
    "    b2 = BLEU_score(source, target,2)\n",
    "    b3 = BLEU_score(source, target,3)\n",
    "    b4 = BLEU_score(source, target,4)\n",
    "    return 0.5 * b1 + 0.5 * b2 + b3 + b4\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"\n",
    "    model: CaptioningRNN model\n",
    "    Prints unigram BLEU score averaged over 1000 training and val examples.\n",
    "    \"\"\"\n",
    "    BLEUscores = {}\n",
    "    for split in ['train', 'val']:\n",
    "        minibatch = sample_coco_minibatch(data, split=split, batch_size=1000)\n",
    "        gt_captions, features, urls = minibatch\n",
    "        gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "        sample_captions = model.sample(features)\n",
    "        sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "        total_score = 0.0\n",
    "        for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "            total_score += BLEU_score(gt_caption, sample_caption)\n",
    "\n",
    "        BLEUscores[split] = total_score / len(sample_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))\n",
    "        \n",
    "# ref, hypo = metrics.load_textfiles(tru_caps, gen_caps)\n",
    "# print(metrics.score(ref, hypo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Baseline (Trained only on BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b23dc119e944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# baseline.load_state_dict(torch.load('./pretrained/rewardNetwork_pre.pt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBaselineNetworkRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordvec_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# baseline = RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "# baseline.load_state_dict(torch.load('./pretrained/rewardNetwork_pre.pt'))\n",
    "\n",
    "class BaselineNetworkRNN(nn.Module):\n",
    "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=512, hidden_dim=512, dtype=np.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        self.hidden_cell = torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "        \n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        self.gru = nn.GRU(wordvec_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        output, self.hidden_cell = self.gru(input_captions.view(len(input_captions) ,1, -1), self.hidden_cell)\n",
    "        return output\n",
    "    \n",
    "class BaselineNetwork(nn.Module):\n",
    "    def __init__(self, word_to_idx, batch_size=10, wordvec_dim=512):\n",
    "        super().__init__()\n",
    "        vocab_size = len(word_to_idx)\n",
    "        self.batch_size = batch_size\n",
    "        self.caption_embedding = nn.Embedding(vocab_size, wordvec_dim)\n",
    "        \n",
    "        self.scoring1 = nn.Linear(16 * 512, 512)\n",
    "        self.scoring2 = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, captions, train=True):\n",
    "        \n",
    "        input_captions = self.caption_embedding(captions)\n",
    "        if train == False:\n",
    "            margin = torch.zeros(1, 16 - input_captions.shape[1], 512).to(device)\n",
    "            input_captions = torch.cat((input_captions, margin), dim=1)\n",
    "            input_captions = input_captions.view(1, -1)\n",
    "          \n",
    "        else:\n",
    "            margin = torch.zeros(self.batch_size, 16 - input_captions.shape[1], 512).to(device)\n",
    "            input_captions = torch.cat((input_captions, margin), dim=1)\n",
    "            input_captions = input_captions.view(self.batch_size, -1)\n",
    "        output = self.scoring1(input_captions)\n",
    "        output = self.scoring2(output)\n",
    "        output = output.squeeze(1)\n",
    "        return output\n",
    "\n",
    "baseline = BaselineNetwork(data[\"word_to_idx\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4ff51fce1059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# policyNet.load_state_dict(torch.load('./pretrained/policyNetwork_pre2.pt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "# policyNet.load_state_dict(torch.load('./pretrained/policyNetwork_pre2.pt'))\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(baseline.parameters(), lr=0.001)\n",
    "\n",
    "caps0 = []\n",
    "caps1 = []\n",
    "\n",
    "f = open(\"./results/truth3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps0.append(x)\n",
    "f = open(\"./results/greedy3.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    caps1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0964567e2ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "bestLoss = 10\n",
    "batch_size = 10\n",
    "\n",
    "baseline.train()\n",
    "for epoch in range(epochs):\n",
    "    caps = []\n",
    "    bs = []\n",
    "    \n",
    "    i = epoch\n",
    "    for _ in range(batch_size):\n",
    "        i %= len(caps0)\n",
    "        b1 = BLEU_score(caps0[i], caps1[i], 1)\n",
    "        b2 = BLEU_score(caps0[i], caps1[i], 2)\n",
    "        b3 = BLEU_score(caps0[i], caps1[i], 3)\n",
    "        b4 = BLEU_score(caps0[i], caps1[i], 4)\n",
    "        b = 0.5 * b1 + 0.5 * b2 + b3 + b4\n",
    "        bs.append(b)\n",
    "        \n",
    "        cap = caps1[i].split()\n",
    "        for j in range(len(cap)):\n",
    "            cap[j] = data[\"word_to_idx\"][cap[j]]\n",
    "        while(len(cap) < 16):\n",
    "            cap.append(0)\n",
    "        \n",
    "        caps.append(cap)\n",
    "        i += 1\n",
    "        \n",
    "    caps = torch.tensor(caps).to(device)\n",
    "    bs = torch.tensor(bs).to(device)\n",
    "    \n",
    "    output = baseline.forward(caps)\n",
    "    if (epoch == epochs - 1):\n",
    "        print(output, bs)\n",
    "    print(loss.item())\n",
    "    loss = criterion(output, bs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if loss.item() < bestLoss or (epoch == epochs-1 and bestLoss == 10):\n",
    "        bestLoss = loss.item()\n",
    "        torch.save(baseline.state_dict(), \"./pretrained/baseline_pre.pt\")\n",
    "        print(\"Best! epoch:\", epoch, \"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Advantage Actor Critic Model for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-903984c81043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdvantageActorCriticNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicyNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdvantageActorCriticNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalueNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalueNet\u001b[0m \u001b[0;31m#RewardNetwork(data[\"word_to_idx\"]).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class AdvantageActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, valueNet, policyNet):\n",
    "        super(AdvantageActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.valueNet = valueNet #RewardNetwork(data[\"word_to_idx\"]).to(device)\n",
    "        self.policyNet = policyNet #PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Get value from value network\n",
    "        values = self.valueNet(features, captions)\n",
    "        # Get action probabilities from policy network\n",
    "        probs = self.policyNet(features.unsqueeze(0), captions)[:, -1:, :]        \n",
    "        return values, probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'PolicyNetwork' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-986860557b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicyNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalueNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaselineNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pretrained/policyNetwork_pre2.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PolicyNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "baseline = BaselineNetwork(data[\"word_to_idx\"]).to(device)\n",
    "\n",
    "if (os.path.exists('./pretrained/policyNetwork_pre2.pt')):\n",
    "    policyNet.load_state_dict(torch.load('./pretrained/policyNetwork_pre2.pt'))\n",
    "    print(\"policy pre2 loaded\")\n",
    "else:\n",
    "    policyNet.load_state_dict(torch.load('./pretrained/policyNetwork_pre.pt'))\n",
    "\n",
    "if (os.path.exists('./pretrained/valueNetwork_pre2.pt')):\n",
    "    valueNet.load_state_dict(torch.load('./pretrained/valueNetwork_pre2.pt'))\n",
    "    print(\"valid pre2 loaded\")\n",
    "else:\n",
    "    valueNet.load_state_dict(torch.load('./pretrained/valueNetwork_pre.pt'))\n",
    "\n",
    "baseline.load_state_dict(torch.load('./pretrained/baseline_pre.pt'))\n",
    "    \n",
    "a2cNetwork = AdvantageActorCriticNetwork(valueNet, policyNet)\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8ddb11a137f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3)\n",
    "word_to_idx = {w : i for i, w in data[\"word_to_idx\"].items()}\n",
    "b = word_to_idx[a.item()]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_coco_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-412e659675b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m   \u001b[0;31m#1000 but out of memory...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msmall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_coco_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2cNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_coco_data' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 50\n",
    "epochs = 50   #1000 but out of memory...\n",
    "small_data = load_coco_data(max_train=50000)\n",
    "bestLoss = 1.0\n",
    "optimizer = optim.Adam(a2cNetwork.parameters(), lr=0.0001)\n",
    "    \n",
    "for epoch in range(epochs): \n",
    "    episodicAvgLoss = 0\n",
    "\n",
    "    captions, features, _ = sample_coco_minibatch(small_data, batch_size=episodes, split='train')\n",
    "    features = torch.tensor(features, device=device).float() \n",
    "    captions = torch.tensor(captions, device=device).long()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        caplen = np.nonzero(captions[episode] == 2)[0][0] + 1\n",
    "        gen_cap = 0\n",
    "        \n",
    "\n",
    "        captions_in = captions[episode:episode+1, :2]\n",
    "        features_in = features[episode:episode+1]\n",
    "\n",
    "        value, probs = a2cNetwork(features_in, captions_in)\n",
    "        \n",
    "        while(captions_in.shape[1] < 16 and gen_cap != 2): #Generate Sequence g1:T \n",
    "\n",
    "            value, probs = a2cNetwork(features_in, captions_in)\n",
    "            probs = F.softmax(probs, dim=2)\n",
    "\n",
    "            dist = probs.cpu().detach().numpy()[0,0]\n",
    "            action = np.random.choice(probs.shape[-1], p=dist)\n",
    "            \n",
    "            gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "            captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "            \n",
    "        print(captions_in)\n",
    "        len_caption = captions_in.shape[1]\n",
    "        \n",
    "        Qs = [0] * len_caption\n",
    "        Bs = [0] * len_caption\n",
    "        Ps = []\n",
    "        \n",
    "        for t in range(len_caption):  # Compute the Q and B for every t in range from 1 to T\n",
    "            captions_in = captions[episode:episode+1, : t+1]\n",
    "            features_in = features[episode:episode+1]\n",
    "            b = baseline.forward(captions_in, train=False)\n",
    "            gen_cap = 0\n",
    "            first = 1\n",
    "\n",
    "            while(captions_in.shape[1] < 16 and gen_cap != 2): #Generate Sequence g1:T  for calculating Q\n",
    "                \n",
    "                value, probs = a2cNetwork(features_in, captions_in)\n",
    "\n",
    "                probs = F.softmax(probs, dim=2)\n",
    "                \n",
    "                dist = probs.cpu().detach().numpy()[0,0]\n",
    "                action = np.random.choice(probs.shape[-1], p=dist)\n",
    "\n",
    "                gen_cap = torch.from_numpy(np.array([action])).unsqueeze(0).to(device)\n",
    "                captions_in = torch.cat((captions_in, gen_cap), axis=1)\n",
    "\n",
    "                if(first == 1):\n",
    "                    first = 0\n",
    "                    print(\"probs\",probs[0,0,action])\n",
    "                    Ps.append(probs[0, 0, action])\n",
    "\n",
    "                    \n",
    "            \n",
    "        \n",
    "            target_caption = captions[episode:episode+1]\n",
    "            word_to_idx = {w : i for i, w in data[\"word_to_idx\"].items()}\n",
    "                \n",
    "            q = BLEU_tensor(captions_in, target_caption)\n",
    "            \n",
    "            Qs[t] = q\n",
    "            Bs[t] = b.item()\n",
    "            \n",
    "        if (len(Ps)==0): continue\n",
    "        Ps = torch.tensor(Ps, dtype=torch.float32, device=device, requires_grad=True)\n",
    "        Qs = torch.tensor(Qs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "        Bs = torch.tensor(Bs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "        \n",
    "        loss_V = 0\n",
    "        loss_L = 0\n",
    "        for t in range(len(Ps)):\n",
    "            loss_V += Ps[t] * torch.log(Ps[t]) * (Qs[t] - Bs[t])\n",
    "            loss_L += 0.1 * (Qs[t] - Bs[t])**2\n",
    "        \n",
    "        loss = loss_V + loss_L\n",
    "        \n",
    "        episodicAvgLoss += loss.item()/episodes\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"epoch:\", epoch, episodicAvgLoss)\n",
    "        \n",
    "\n",
    "# bestLoss = episodicAvgLoss\n",
    "# torch.save(policyNet.state_dict(), \"./pretrained/policyNetwork_final2.pt\")\n",
    "# torch.save(valueNet.state_dict(), \"./pretrained/valueNetwork_final2.pt\")\n",
    "# # torch.save(rewardNet.state_dict(), \"./pretrained/rewardNetwork_final.pt\")\n",
    "# print(\"Models are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'PolicyNetwork' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a12b61a8e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicyNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpolicyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pretrained/policyNetwork_pre2.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalueNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_to_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalueNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pretrained/valueNetwork_pre2.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PolicyNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "policyNet = PolicyNetwork(data[\"word_to_idx\"]).to(device)\n",
    "policyNet.load_state_dict(torch.load('./pretrained/policyNetwork_pre2.pt'))\n",
    "valueNet = ValueNetwork(data[\"word_to_idx\"]).to(device)\n",
    "valueNet.load_state_dict(torch.load('./pretrained/valueNetwork_pre2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/policyvalue4.txt', 'w'):\n",
    "    pass\n",
    "with open('results/truth4.txt', 'w'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e4af28519e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_of_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_coco_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "num_of_sentences = 150\n",
    "\n",
    "with torch.no_grad():\n",
    "    max_seq_len = 17\n",
    "    captions, features, urls = sample_coco_minibatch(small_data, batch_size=1000, split='val')\n",
    "    gen_caps = []\n",
    "    tru_caps = []\n",
    "    with open('results/policyvalue4.txt', mode='w') as f:\n",
    "        with open('results/truth4.txt', mode='w') as f2:\n",
    "            count = 0\n",
    "            \n",
    "            for i in range(num_of_sentences):\n",
    "                try:    #pass if the url doesnt exist\n",
    "                    plt.imshow(image_from_url(urls[i]))\n",
    "                    count += 1\n",
    "                except:\n",
    "                    continue\n",
    "                if count >= 101: \n",
    "                    break \n",
    "                decoded_tru_caps = decode_captions(captions[i], data[\"idx_to_word\"])\n",
    "                gen_cap = GenerateCaptionsWithBeamSearchValueScoring(features[i:i+1], captions[i:i+1], policyNet)[0][0][0]\n",
    "                decoded_gen_caps = decode_captions(gen_cap, data[\"idx_to_word\"])\n",
    "\n",
    "\n",
    "                f.writelines(' '.join(decoded_gen_caps))\n",
    "                f.write('\\n')\n",
    "\n",
    "                f2.writelines(' '.join(decoded_tru_caps))\n",
    "                f2.write('\\n')\n",
    "                \n",
    "                print(\"Truth   : \", decoded_tru_caps) \n",
    "                print(\"Output: \", decoded_gen_caps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_caps = []\n",
    "gen_caps = []\n",
    "\n",
    "f = open(\"./results/truth4.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    tru_caps.append(x)\n",
    "    \n",
    "f = open(\"./results/policyvalue4.txt\", \"r\")\n",
    "for x in f:\n",
    "    x = \" \".join([w for w in x.split(' ') if ('<END>' not in w and '<START>' not in w and '<UNK>' not in w)])\n",
    "    gen_caps.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BLEU-1 : 0\nBLEU-2 : 0\nBLEU-3 : 0\nBLEU-4 : 0\n"
    }
   ],
   "source": [
    "bleu = 0\n",
    "for w in range(1, 5):\n",
    "    for i in range(len(tru_caps)):\n",
    "\n",
    "        bleu += BLEU_score(tru_caps[i], gen_caps[i], w)\n",
    "        bleu /= len(tru_caps)\n",
    "\n",
    "    print(\"BLEU-\" + str(w), \":\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6c30e72f6c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_textfiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtru_caps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_caps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "ref, hypo = metrics.load_textfiles(tru_caps, gen_caps)\n",
    "print(metrics.score(ref, hypo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1594123292754"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}